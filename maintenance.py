# -*- coding: utf-8 -*-
"""Maintenance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14UG7tLM0p4k8AiDs7dm56Hg_XIem-qKU
"""

import pandas as pd

df = pd.read_csv('/content/dataset.csv')

df.shape

#finding duplicates values
df.duplicated().sum()

df.head()

import pandas as pd

# Assuming 'df' is your DataFrame
gt_state_counts = df['GT_state'].value_counts()

# Print the counts of each unique value in 'GT_state'
print(gt_state_counts)

# Calculate the percentage of each unique value
gt_state_percentages = (gt_state_counts / len(df)) * 100
print(gt_state_percentages)

# Check for missing values in each column
missing_values = df.isnull().sum()

# Print the result
print(missing_values)

import pandas as pd
import numpy as np

def detect_outliers_iqr(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    return outliers

# Assuming 'df' is your DataFrame
numerical_features = df.select_dtypes(include=np.number).columns
numerical_features = numerical_features.drop('GT_state')

for feature in numerical_features:
    outliers = detect_outliers_iqr(df[feature])
    outlier_count = len(outliers)
    print(f"Feature: {feature}, Outlier Count: {outlier_count}")

from scipy.stats.mstats import winsorize

df['Injecton_control'] = winsorize(df['Injecton_control'], limits=[0.05, 0.05])

#Scaling

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Assuming you have loaded your dataset into the 'data' DataFrame
# Example: data = pd.read_csv('your_dataset.csv')

# Separate features (X) and target (y)
X = df.iloc[:, :-1].values  # All columns except the target column (features)
y = df.iloc[:, -1].values   # The target column (GT_state)

# Apply standardization to the input features (excluding the target)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create a DataFrame for the cleaned, scaled features
X_scaled_df = pd.DataFrame(X_scaled, columns=df.columns[:-1])  # Keep column names for features

# Re-add the unscaled target (GT_state) to the cleaned dataset
cleaned_data = pd.concat([X_scaled_df, pd.Series(y, name='GT_state')], axis=1)

# Save the cleaned dataset to a CSV file
cleaned_data.to_csv('cleaned_dataset.csv', index=False)

# Download the file to your local system
from google.colab import files
files.download('cleaned_dataset.csv')

#1) get data ready for training

import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split

# Step 1: Load data as DataFrame
# Assuming the data is in a CSV file (replace 'pm.csv' with the actual path)
data = pd.read_csv('/content/cleaned_dataset.csv')

# Separate features and target (first 14 columns are features, the last is the target)
X = data.iloc[:, :-1].values  # First 14 columns are input features
y = data.iloc[:, -1].values   # Last column is the target variable (GT_state)

# Convert to NumPy arrays
X = np.array(X)
y = np.array(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert the NumPy arrays to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Reshape to 2D for PyTorch
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

# Create DataLoader for training and testing data
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

#2) built neural network

import torch.nn as nn

# Define a simple linear regression model
class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1)  # One input layer, one output

    def forward(self, x):
        return self.linear(x)

# Instantiate the model
model = LinearRegressionModel(input_dim=14)

# Define the loss function (Mean Squared Error Loss) and the optimizer (Adam)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Define hyperparameters
num_epochs = 50  # Adjust this based on the complexity of the task

#3) train the model

# Training loop
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    running_loss = 0.0

    for inputs, targets in train_loader:
        optimizer.zero_grad()  # Zero out the gradients

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Print loss at each epoch
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')

#4) evaluate the model
from sklearn.metrics import mean_squared_error

# Evaluation
model.eval()  # Set the model to evaluation mode
test_loss = 0.0
predictions = []

with torch.no_grad():  # Disable gradient calculation for evaluation
    for inputs, targets in test_loader:
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        test_loss += loss.item()

        predictions.extend(outputs.numpy())

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, np.array(predictions)))

# Print the average loss and RMSE
print(f'Average Test Loss: {test_loss/len(test_loader):.4f}')
print(f'RMSE: {rmse:.4f}')

#Hyperparameter tuning

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error

# Define the improved model with dropout
class ImprovedModel(nn.Module):
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout_rate):
        super(ImprovedModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.fc3 = nn.Linear(hidden_dim2, 1)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        return self.fc3(x)

# Wrapper class to integrate with sklearn's GridSearchCV
class PyTorchModelWrapper:
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout_rate):
        self.input_dim = input_dim
        self.hidden_dim1 = hidden_dim1
        self.hidden_dim2 = hidden_dim2
        self.dropout_rate = dropout_rate
        self.model = ImprovedModel(input_dim, hidden_dim1, hidden_dim2, dropout_rate)
        self.criterion = nn.MSELoss()
        self.optimizer = None

    # Required for GridSearchCV to retrieve model parameters
    def get_params(self, deep=True):
        return {
            'input_dim': self.input_dim,
            'hidden_dim1': self.hidden_dim1,
            'hidden_dim2': self.hidden_dim2,
            'dropout_rate': self.dropout_rate
        }

    # Required for GridSearchCV to set model parameters
    def set_params(self, **params):
        for param, value in params.items():
            setattr(self, param, value)
        # Re-initialize the model with updated parameters
        self.model = ImprovedModel(self.input_dim, self.hidden_dim1, self.hidden_dim2, self.dropout_rate)
        return self

    # Training function
    def fit(self, X, y, epochs=100, batch_size=64):
        X_tensor = torch.tensor(X, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Reshape y to 2D for PyTorch
        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        self.optimizer = optim.Adam(self.model.parameters())

        for epoch in range(epochs):
            for inputs, targets in dataloader:
                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)
                loss.backward()
                self.optimizer.step()

    # Prediction function
    def predict(self, X):
        X_tensor = torch.tensor(X, dtype=torch.float32)
        with torch.no_grad():
            return self.model(X_tensor).numpy()

# Load your cleaned data
data = pd.read_csv('/content/cleaned_dataset.csv')
X = data.iloc[:, :-1].values  # Input features
y = data.iloc[:, -1].values   # Target variable (GT_state)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'hidden_dim1': [64, 128, 256],
    'hidden_dim2': [32, 64, 128],
    'dropout_rate': [0.1, 0.2, 0.3],
}

# Create the model wrapper for GridSearchCV
model_wrapper = PyTorchModelWrapper(input_dim=X_train.shape[1], hidden_dim1=128, hidden_dim2=64, dropout_rate=0.2)

# Create GridSearchCV object
grid_search = GridSearchCV(
    estimator=model_wrapper,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',  # Use negative MSE for regression
    cv=5,  # 5-fold cross-validation
    n_jobs=-1,  # Use all available processors
    verbose=2
)

# Fit the grid search to the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score (Negative MSE):", best_score)

# Evaluate the model with the best parameters on the test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)  # Predict on the test set

# Calculate RMSE on the test set
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'RMSE on Test Set: {rmse:.4f}')

#5) improve the model with experiments
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error


# Define the improved model
class ImprovedModel(nn.Module):
    def __init__(self, input_dim):
        super(ImprovedModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)  # Increased hidden units
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.dropout = nn.Dropout(0.2)  # Added dropout for regularization

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)  # Apply dropout after activation
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        return self.fc3(x)

# Instantiate the improved model
improved_model = ImprovedModel(input_dim=14)
optimizer = torch.optim.Adam(improved_model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Training loop
num_epochs = 100  # Increased epochs
batch_size = 64  # Adjusted batch size

train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).unsqueeze(1))
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)

for epoch in range(num_epochs):
    improved_model.train()
    running_loss = 0.0
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = improved_model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')

# Evaluation
improved_model.eval()
test_loss = 0.0
predictions = []

test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1))
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = improved_model(inputs)
        loss = criterion(outputs, targets)
        test_loss += loss.item()
        predictions.extend(outputs.numpy())

rmse = np.sqrt(mean_squared_error(y_test, np.array(predictions)))

print(f'Average Test Loss: {test_loss/len(test_loader):.4f}')
print(f'RMSE: {rmse:.4f}')

